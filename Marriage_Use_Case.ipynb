{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading the necessary packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "from datetime import datetime,timedelta,date\n",
    "from functools import reduce\n",
    "from dateutil.relativedelta import *\n",
    "from warnings import filterwarnings\n",
    "filterwarnings(\"ignore\")\n",
    "##Dataframe Global variables\n",
    "pd.set_option('display.max_columns',40)\n",
    "pd.set_option('max_colwidth',50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file(filepath,filetype,encoding='ISO-8859-1'):\n",
    "    \"\"\"\n",
    "    This function reads txt and csv files and returns the data\n",
    "    \"\"\"\n",
    "    if(filetype==\"txt\" or \"csv\"):\n",
    "        data = pd.read_csv(filepath,encoding=encoding,low_memory=False)    \n",
    "    else:\n",
    "        raise Exception(\"Kindly Check! Only txt or csv filetype allowed\")\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file_chunks(filepath,filetype,chunk_size,encoding='ISO-8859-1'):\n",
    "    \"\"\"\n",
    "    This function reads data from the file in chunks\n",
    "    \"\"\"\n",
    "    if(filetype==\"txt\" or \"csv\"):\n",
    "        chunk_data = pd.read_csv(filepath,encoding=encoding,low_memory=False,chunksize=chunk_size)\n",
    "        for chunks in range(0,999):\n",
    "            if(chunks==0):\n",
    "                card_txn_data = next(chunk_data)\n",
    "            else:\n",
    "                try:\n",
    "                    card_txn_data = card_txn_data.append(next(chunk_data))\n",
    "                except StopIteration:\n",
    "                    break\n",
    "    else:\n",
    "        raise Exception(\"Kindly Check! Only txt or csv filetype allowed\")\n",
    "    return card_txn_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clothes_cats = ['women s ready to wear stores','women s accessory and specialty shops','miscellaneous apparel and accessory shops','family clothing stores','miscellaneous and specialty retail stores',\\\n",
    "#'men s and women s clothing stores','men s and boy s clothing and accessories stores','men s women s and children s uniforms and commercial clothing','clothing rental costumes formal wear uniforms']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "required_cols = ['ArtificialAccountKey', 'POSTAMOUNT','CARDACCEPTORCITY',\n",
    "       'CARDACCEPTORCOUNTRY', 'CARDACCEPTORNAME', 'CARDACCEPTORSTATE',\n",
    "       'CARDACCEPTORSTREET', 'LOCALTRANDATE', 'LOCALTRANTIME',\n",
    "       'MerchantCategory','MISCDATA3','OURACCTCODE1', 'OURCARDTYPE', 'OURSYSTEMDATE', 'OURTRANCODE',\n",
    "       'RECURRINGFLAG']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunksize = 350000\n",
    "## Transactions of customers for past one year\n",
    "#filepath = r\"C:\\Users\\sugan\\Desktop\\USF Grow Financial\\MajorEvents\\USFCardTransactionsLimitedMembers20181107.txt\"\n",
    "filepath = r\"C:\\Users\\ujjwa\\Downloads\\Grow Backup\\Grow\\data\\Customers_10000\\Customers_10000.csv\"\n",
    "filetype = 'csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading the data\n",
    "customer_data = read_file_chunks(filepath,filetype,chunksize)\n",
    "customer_data=customer_data[required_cols]\n",
    "##check if data is loaded successfully\n",
    "if(len(customer_data)==0):\n",
    "    raise Exception(\"Data not loaded Kindly check!format of the data file\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "customer_data['LOCALTRANDATE'] = pd.to_datetime(customer_data['LOCALTRANDATE'])\n",
    "customer_data['MONTH'] = customer_data.LOCALTRANDATE.apply(lambda x:format(x.month,'02'))\n",
    "customer_data['YEAR'] = customer_data.LOCALTRANDATE.apply(lambda x:x.year)\n",
    "customer_data['MO_YR'] = customer_data['YEAR'].astype('str')+'_'+customer_data['MONTH'].astype('str')\n",
    "customer_data['DATE'] = customer_data['LOCALTRANDATE'].apply(lambda x:datetime.strftime(x,'%Y-%m-%d'))\n",
    "customer_data.sort_values(by=['ArtificialAccountKey','LOCALTRANDATE']).reset_index(inplace=True,drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Create a new cleaned card acceptor names and merchant category\n",
    "def clean_name(text,space=''):\n",
    "    text = str(text).lower()\n",
    "    temp_text = re.sub('[^a-zA-Z\\s]',\"\",text)\n",
    "    if(space == \"X\"):\n",
    "        temp_text = temp_text.strip()\n",
    "    else:\n",
    "        temp_text = temp_text.replace(\" \",\"\")\n",
    "    return temp_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "customer_data['clean_cardacceptor_name'] = customer_data.CARDACCEPTORNAME.apply(lambda x:clean_name(str(x), 'X'))\n",
    "customer_data['clean_merchant'] = customer_data.MerchantCategory.apply(lambda x:clean_name(str(x),'X'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Grouped the data into three categories based on keywords, merchant type and posted amount and assign a probability score for each.\n",
    "'''\n",
    "1.Group 1: Customers who have made large transaction on relevant categories (hotels, jewelry, catering, florals) in a month.\n",
    "2.Group 2: Customers who have spent on bridal stores >300$\n",
    "3.Group 3: Customers who have spent on photography > 500$\n",
    "4.Mark probaibility as 'Low' for Group 2 & 3 and 'None' for Group 1.\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Large Transactions (Hotels, Jewelary, Catering, Florists) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Keyword and their categories for bridal stores and large categories.\n",
    "keywords = pd.ExcelFile(r\"C:\\Users\\ujjwa\\Downloads\\Grow Backup\\Grow\\data\\Wedding_Keywords.xlsx\")\n",
    "bridal_stores = keywords.parse(sheet_name='bride', names=['keyword'])\n",
    "large_merchant = keywords.parse(sheet_name='large', names=['key','search_term'])\n",
    "\n",
    "##Create a dictionary with search terms as key and actual names as value\n",
    "dict_large = {}\n",
    "for index,row in large_merchant.iterrows():\n",
    "    dict_large[row['search_term']]= row['key']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tagging records for large transactions.\n",
    "final_class = []\n",
    "for index,row in customer_data.iterrows():\n",
    "    name = str(row['clean_merchant'])\n",
    "    match = []\n",
    "    for key,item in dict_large.items():\n",
    "        if(type(re.search(key,name)) is not type(None)):\n",
    "            match.append(item)\n",
    "        else:\n",
    "            pass\n",
    "    final_class.append(match) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Flattening the predictions list\n",
    "flat_list = []\n",
    "for sublist in final_class:\n",
    "    if(len(sublist)==0):\n",
    "        flat_list.append(\"\")\n",
    "    else:\n",
    "        flat_list.append(sublist[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "##  Group 1: tagging the data with identified large category\n",
    "\n",
    "customer_data['Identified_large_txn'] = pd.Series(flat_list)\n",
    "customer_large_txn = customer_data.loc[customer_data.Identified_large_txn != '']\n",
    "##creating groups\n",
    "cust_large_grp = customer_large_txn.groupby(by=['ArtificialAccountKey','Identified_large_txn','MO_YR'],as_index=False)['POSTAMOUNT'].agg({'spent':'sum'})\n",
    "##splitting groups for hotels, jewelary, florist and caterer and specifying min transaction amount for each category.\n",
    "cust_hotel_df = cust_large_grp.loc[(cust_large_grp.Identified_large_txn == 'hotels') & (cust_large_grp.spent>2000), ['ArtificialAccountKey', 'Identified_large_txn']]\n",
    "cust_jewelry_df = cust_large_grp.loc[(cust_large_grp.Identified_large_txn == 'jewelry') & (cust_large_grp.spent>2000), ['ArtificialAccountKey', 'Identified_large_txn']]\n",
    "cust_florists_df = cust_large_grp.loc[(cust_large_grp.Identified_large_txn == 'florists') & (cust_large_grp.spent>500), ['ArtificialAccountKey', 'Identified_large_txn']]\n",
    "cust_caterer_df = cust_large_grp.loc[(cust_large_grp.Identified_large_txn == 'caterers') & (cust_large_grp.spent>500), ['ArtificialAccountKey', 'Identified_large_txn']]\n",
    "#contactenating all the large transactions\n",
    "cust_large_df = pd.concat([cust_hotel_df,cust_jewelry_df, cust_florists_df, cust_caterer_df])\n",
    "cust_large_df.reset_index(inplace=True,drop=True)\n",
    "cust_large_df['prob'] = ''\n",
    "cust_large_df['category'] = 'large'\n",
    "cust_large_df.rename(columns = {'Identified_large_txn':'clean_cardacceptor_name'}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bridal Stores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Group 2: Bridal Stores\n",
    "other_cat = ['theatrical producers except motion pictures ticket agencies',\n",
    "       'insurance sales underwriting and premiums',\n",
    "        'commercial photography art and graphics',\n",
    "       'card shops gift novelty and souvenir shops']\n",
    "\n",
    "cust_bride_df = pd.DataFrame()\n",
    "for items in bridal_stores.values.tolist():\n",
    "    #print(items)\n",
    "    cust_bride_df = cust_bride_df.append(customer_data.loc[customer_data.clean_cardacceptor_name.str.contains(items[0]) & (~customer_data.clean_merchant.isin(other_cat))])\n",
    "    \n",
    "cust_bride_df.reset_index(inplace=True,drop=True)\n",
    "cust_bride_grp = cust_bride_df.groupby(by=['ArtificialAccountKey','clean_cardacceptor_name','MO_YR'],as_index=False)['POSTAMOUNT'].agg({'spent':'sum'})\n",
    "\n",
    "##customers who spent more than 300 in bridal stores.\n",
    "cust_bride_final = cust_bride_grp.loc[(cust_bride_grp.spent>300), ['ArtificialAccountKey', 'clean_cardacceptor_name']]\n",
    "cust_bride_final.reset_index(inplace=True,drop=True)\n",
    "cust_bride_final['prob'] = 'low'\n",
    "cust_bride_final['category'] = 'bridal'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Photography"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Group 3: Photgraphy\n",
    "photo_cat = ['commercial photography art and graphics']\n",
    "customer_photo = customer_data.loc[(customer_data.clean_merchant.isin(photo_cat)),]\n",
    "customer_photo.reset_index(inplace=True,drop=True)\n",
    "cust_photo_grp = customer_photo.groupby(by=['ArtificialAccountKey','MO_YR', 'clean_cardacceptor_name'],as_index=False)['POSTAMOUNT'].agg({'spent':'sum'})\n",
    "\n",
    "##customers who spent more than 500 in photoshoots.\n",
    "cust_photo_df = cust_photo_grp.loc[(cust_photo_grp.spent>500), ['ArtificialAccountKey', 'clean_cardacceptor_name']]\n",
    "cust_photo_df.reset_index(inplace=True,drop=True)\n",
    "cust_photo_df['prob'] = 'low'\n",
    "cust_photo_df['category'] = 'photography'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concatenation and Scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## concatenate cust_photo_df, cust_bride_df, cust_large_df for creating a dictionary\n",
    "final_df = pd.concat([cust_photo_df, cust_bride_final, cust_large_df])\n",
    "final_df.reset_index(inplace=True,drop=True)\n",
    "\n",
    "#creating a dictionary for tagging and comparing the spent frequencies.\n",
    "cust_dict = {}\n",
    "for index,row in final_df.iterrows():\n",
    "    if(row.ArtificialAccountKey in cust_dict.keys()):\n",
    "        if row.clean_cardacceptor_name not in cust_dict[row.ArtificialAccountKey][0]:\n",
    "            cust_dict[row.ArtificialAccountKey][0].append(row.clean_cardacceptor_name)\n",
    "        if row.category not in cust_dict[row.ArtificialAccountKey][1]:\n",
    "            cust_dict[row.ArtificialAccountKey][1].append(row.category) \n",
    "        if row.prob not in cust_dict[row.ArtificialAccountKey][2]:\n",
    "            cust_dict[row.ArtificialAccountKey][2].append(row.prob)    \n",
    "    else:\n",
    "        cust_dict[row.ArtificialAccountKey] = [[row.clean_cardacceptor_name],[row.category],[row.prob]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Changing the assigned probability with med, high, very high based on the following rules:\n",
    "'''\n",
    "If customer is present in Group 2 and 3 -> High\n",
    "If customer is present in Group 1 and either of Group 2 or 3 -> Medium\n",
    "If customer is present in all the Groups 1, 2 and 3 -> Very_High\n",
    "No matches -> Probability remains unchanged.\n",
    "'''\n",
    "for i in cust_dict.values():\n",
    "    if len(i[1])==2:\n",
    "        ## scored probability as 'High'\n",
    "        if((i[1][0] == 'photography' and i[1][1] == 'bridal') or (i[1][0] == 'bridal' and i[1][1] == 'photography')):\n",
    "            i[2] = ['high']\n",
    "       ## scored probability as 'Medium'\n",
    "        elif((i[1][0] == 'bridal' and i[1][1] == 'large') or (i[1][0] == 'large' and i[1][1] == 'bridal')):\n",
    "            i[2] = ['medium']\n",
    "        elif((i[1][0] == 'photography' and i[1][1] == 'large') or (i[1][0] == 'large' and i[1][1] == 'photography')):\n",
    "            i[2] = ['medium']\n",
    "    elif len(i[1])==3:\n",
    "        ## scored probability as 'Very_High'\n",
    "        if((i[1][0] == 'photography' and i[1][1] == 'bridal' and i[1][2] == 'large') or (i[1][0] == 'photography' and i[1][1] == 'large' and i[1][2] == 'bridal')):\n",
    "            i[2] = ['very_high']\n",
    "        elif((i[1][0] == 'bridal' and i[1][1] == 'photography' and i[1][2] == 'large') or (i[1][0] == 'bridal' and i[1][1] == 'large' and i[1][2] == 'photography')):\n",
    "            i[2] = ['very_high']\n",
    "        elif((i[1][0] == 'large' and i[1][1] == 'photography' and i[1][2] == 'bridal') or (i[1][0] == 'large' and i[1][1] == 'bridal' and i[1][2] == 'photography')):\n",
    "            i[2] = ['very_high']\n",
    "            \n",
    "## converting the dictionary into dataframe\n",
    "output_df = pd.DataFrame()\n",
    "for key,item in cust_dict.items():\n",
    "    customer_key = key\n",
    "    matched_card_names = ','.join(item[0])\n",
    "    matched_category = ','.join(item[1])\n",
    "    prob = item[2]\n",
    "    output_df = pd.concat([output_df,pd.DataFrame({'customer_key':customer_key,'card_acceptor_name':matched_card_names,'category':matched_category,'probability':prob})])\n",
    "\n",
    "## Eliminating other unscored customers\n",
    "final_scored_df = output_df.loc[output_df.probability != \"\"]    \n",
    "final_scored_df.reset_index(inplace=True,drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## output - list of customers scored with probability of marriage.\n",
    "final_scored_df   "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
